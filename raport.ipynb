{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be016e7a",
   "metadata": {},
   "source": [
    "**1. Wstęp**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ae4f5",
   "metadata": {},
   "source": [
    "W ramch projketu opracowalśmy model regresyjny do prognozowania cen wynajmu mieszkań w Poznaniu. Zastosowaliśmy biblioteki LightGBM oraz XGBoost wsparte imputacją braków danych (KNN) oraz transformacją logarytmiczną zmiennej celu. Nasza grupa na Kaggle nazywała się Martyna & Piotr (#3) z wynikiem 78508.79 uzyskanym LightGBM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0810802",
   "metadata": {},
   "source": [
    "**2. Metodyka**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3657c3cf",
   "metadata": {},
   "source": [
    "2.1 Przygotowanie danych zaczeliśmy od standaryzacji i oczyszczenia zbioru z błędnych wartości. \n",
    " - Zidentyfikowaliśmy wartości -999 oraz -9, które w zbiorze służyły jako oznaczenie braków danych. Zostały one zmienone na NAN, aby umożliwić poprawną imputację.\n",
    " - Stworzyliśmy pętlę weryfikującą poprawność danych numerycznych, gdzie wszytskie wartości ujemne zmienione zostały na brak danych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26916416",
   "metadata": {},
   "source": [
    "2.2 Następnie zastosowalismy dwuetapową imputcję danych.\n",
    "- Na początku odzyskaliśmy dane z tytułów ogłoszeń. Stworzyliśmy funkcję opartą na wyrażeniach reguralnych, które wyodrębniały z tekstu np. powierzchnię. Pozwoliło nam to uzupełnić 400 braków w flat_area, 1531 w flat_rooms oraz 569 w quarter.\n",
    "- Dla pozostałych danych numerycznych oraz kategorycznych, których nie udało się odzyskać zastosowano algorytmy.\n",
    "    - Zmienne numeryczne uzupełniono za pomocą KNN, gdzie braki uzupełniano średnią wartością od 5 najbardziej podobnych sąsiadów.\n",
    "    - Dla zmiennych binarnych zastosowano imputację opartą na regresji logistycznej, która dla każdej cechy trenowała oddzielny model klasyfikacyjny. Braki przewidywano na podsatwie parametrów fizycznych, a w przypadkach, w których model nie mógł zadziałać stosowano modę.\n",
    "    - Do imputacji zmiennej quarter opracowano dedykowaną procedurę imputacji opartą na KNN. Dzielnice zostały zakodowane numerycznie, a brakujące wartości oszacowano na podstawie 5 najbardziej podobnych ofert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cd5911",
   "metadata": {},
   "source": [
    "2.3 Przed trenowaniem modeli dokonaliśmy analizy rozkładu zmiennej celu (price), która wykazała silną skośność prawostronną (drogie apartamenty jako wartości odstające). Aby poprawić stabilnośc procesu oraz zminimalizować wpływ outlinerów zastosowalismy transformację logistyczną. W implementacji wykorzystano wrapper TransformedTargetRegressor z biblioteki Scikit-learn. Pozwoliło to na automatyczne logarytmowanie danych treningowych oraz przekształcenie predykcji do oryginalnej skali cenowej, co uprościło przetwarzanie oraz ewaluacje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b08780",
   "metadata": {},
   "source": [
    "2.4 W etapie modelowania zastosowaliśmy algorytmy oparte na wzmacnianiu gradientowym, w tym XGBoost oraz LightGBM w różnych konfiguracjach. Ostatecznie najlepsze rezultaty osiągnięto przy użyciu LightGBM bez logarytmicznej transformacji zmiennej celu. Model ten wykazał się wysoką skutecznością w wyłapywaniu nieliniowych zależności między cechami, a ceną wynajmu. Zwycięski algorytm Gradient Boosting Decision Tree miał kluczowe parametry wybrane drogą eksperymentalną. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b88b539",
   "metadata": {},
   "source": [
    "**3. Wyniki**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420d100a",
   "metadata": {},
   "source": [
    "3.1 W trakcie prac nad projketem przetestowliśmy cztery główne konfiguracje modeli, gdzie miarą błędu było RMSE. Najlepsze uzyskane wyniki z każdego moelu przedstawia tabela poniżej. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb3da7",
   "metadata": {},
   "source": [
    "| Model | Konfiguracja | Wynik (RMSE) | Komentarz |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **XGBoost** | Baseline, domyślne parametry | **80121.76** | Dobry punkt wyjścia. |\n",
    "| **XGBoost_log** | Transformacja logarytmiczna celu | **81842.94** | Zastosowanie logarytmizacji pogorszyła wynik. |\n",
    "| **LightGBM_log** | Transformacja logarytmiczna celu | **81987.70** | Transformacja również nie przyniosła poprawy. |\n",
    "| **LightGBM** | **Brak transformacji, learning_rate=0.05** | **78508.79** | Odpowiednie dostrojenie dało najmniejszy błąd predykcji |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb020c75",
   "metadata": {},
   "source": [
    "3.2 Proces dochodzenia do najlepszego wyniku przebiegał wieloetapowo.\n",
    "- Baseline: Uruchomienie modelu XGBoost na domyślnych parametrach pozwolił ustalić punkt odniesienia z wynikiem RMSE na poziomie 80121. Był to dobry start, który wskazywał możliwość dalszej optymalizacji. \n",
    "- Zastosowanie transformacji: W celu zniwelowania skośności rozkładu cen zastosowaliśmy transformację logarytymiczną. Ku naszemu zaskoczeniu zabieg ten pogorszył wynik dla obu testowanych przez nas algorytmów. Sugeruje nam to, że błędy estymacji popełniane na skali logarytmicznej, po powrotym prszekształceniu uległy wzmocnieniu. Wynika z tego, że transformacja okazała się niekorzystna.\n",
    "- Zmiana na LightGBM i optymalizacja: Mimo braku poprawy po transformacji danych zmiana algorytmu na LightGBM na surowych danych pzyniosła poprawę. Dzięki szybszemu procesowi uczenia mogliśmy przetestować większy zakres hiperparametrów.\n",
    "- Ostateczna konfiguracja: Odpowiednio wyregulowany model gradientowy potrafił samodzielnie i skutecznie zamodelować dane surowe, bez konieczności stosowania transformacji matematycznych. Dzięki temu model LightGBM uzyskał wynik o ponad 1000pkt lepszy od baseline'u."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b450c6a8",
   "metadata": {},
   "source": [
    "**4. Podsumowanie**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3e9678",
   "metadata": {},
   "source": [
    "Realizacja projektu dostarczyła kilku kluczowych informacji. Największym zaskoczeniem był fakt, że transformacja logarytmiczna zmiennej celu pogorszyła wyniki. Nauczyło nas to, że teoretyczne założenia (o normalizacji rozkładu cen) należy zawsze weryfikować eksperymentalnie. Dobrze dostrojony model na surowych danych (LightGBM) poradził sobie lepiej z wartościami odstającymi niż model z wymuszoną transformacją. Dodatkowo kluczem do sukcesu nie był sam algorytm, ale przygotowanie danych. \"Uratowanie\" wielu rekordów poprzez wyciągnięcie ichz tytułów ogłoszeń dało modelowi więcej informacji niż jakakolwiek zmiana hiperparametrów. Co prowadzi nas do wniosków, że prawidłowa imputacja danych jest kluczowa do stworzenia dobrego modelu. Uzupełnienie ich za pomocą Regex czy KNN, zamiast usuwania braków pozwoliło nam zachować wiele rekordów co przy modelowaniu ceny było bardzo pomocne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b648938",
   "metadata": {},
   "source": [
    "**5. Kod do odtworzenia wyników**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
